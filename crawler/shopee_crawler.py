import requests
import json
import time
import re
import os
from typing import List, Dict, Optional
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from models.product import Product

class ShopeeCrawler:
    """Crawler ƒë·ªÉ l·∫•y d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ Shopee s·ª≠ d·ª•ng Selenium"""
    
    BASE_URL = "https://shopee.vn"
    COOKIES_FILE = "shopee_cookies.json"
    
    def __init__(self, headless: bool = True):
        """
        Kh·ªüi t·∫°o crawler
        headless: True ƒë·ªÉ ch·∫°y browser ·∫©n, False ƒë·ªÉ hi·ªÉn th·ªã browser
        """
        self.headless = headless
        self.driver = None
        self._init_driver()
        self._load_cookies()
    
    def _init_driver(self):
        """Kh·ªüi t·∫°o Selenium WebDriver"""
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument('--headless=new')  # D√πng headless m·ªõi
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('--disable-web-security')
        chrome_options.add_argument('--disable-features=IsolateOrigins,site-per-process')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # Enable performance logging ƒë·ªÉ intercept network requests
        chrome_options.set_capability('goog:loggingPrefs', {'performance': 'ALL'})
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            # Set window size
            self.driver.set_window_size(1920, 1080)
        except Exception as e:
            print(f"L·ªói kh·ªüi t·∫°o Chrome driver: {e}")
            print("ƒê·∫£m b·∫£o ƒë√£ c√†i ƒë·∫∑t Chrome v√† ChromeDriver")
            raise
    
    def _load_cookies(self):
        """Load cookies t·ª´ file n·∫øu c√≥"""
        if os.path.exists(self.COOKIES_FILE):
            try:
                # Truy c·∫≠p trang ch·ªß tr∆∞·ªõc
                self.driver.get(self.BASE_URL)
                time.sleep(2)
                
                with open(self.COOKIES_FILE, 'r', encoding='utf-8') as f:
                    cookies = json.load(f)
                    
                # X√≥a cookies c≈© tr∆∞·ªõc
                self.driver.delete_all_cookies()
                
                # Load cookies m·ªõi
                loaded_count = 0
                for cookie in cookies:
                    try:
                        # ƒê·∫£m b·∫£o domain ƒë√∫ng
                        if 'domain' in cookie:
                            # Ch·ªânh domain n·∫øu c·∫ßn
                            if cookie['domain'].startswith('.'):
                                cookie['domain'] = cookie['domain'][1:]
                        self.driver.add_cookie(cookie)
                        loaded_count += 1
                    except Exception as e:
                        continue
                
                if loaded_count > 0:
                    print(f"‚úÖ ƒê√£ load {loaded_count}/{len(cookies)} cookies t·ª´ file")
                    # Refresh ƒë·ªÉ √°p d·ª•ng cookies
                    self.driver.refresh()
                    time.sleep(2)
                    return True
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load cookies: {e}")
        return False
    
    def _save_cookies(self):
        """L∆∞u cookies v√†o file"""
        try:
            if self.driver:
                cookies = self.driver.get_cookies()
                with open(self.COOKIES_FILE, 'w', encoding='utf-8') as f:
                    json.dump(cookies, f, ensure_ascii=False, indent=2)
                print(f"‚úÖ ƒê√£ l∆∞u {len(cookies)} cookies v√†o {self.COOKIES_FILE}")
        except Exception as e:
            # Kh√¥ng in l·ªói n·∫øu driver ƒë√£ ƒë√≥ng
            pass
    
    def close(self):
        """ƒê√≥ng driver v√† l∆∞u cookies"""
        if self.driver:
            try:
                # L∆∞u cookies tr∆∞·ªõc khi ƒë√≥ng
                self._save_cookies()
            except:
                pass
            try:
                self.driver.quit()
            except:
                try:
                    self.driver.close()
                except:
                    pass
            self.driver = None
    
    def __del__(self):
        """ƒê√≥ng driver khi h·ªßy object"""
        self.close()
    
    def crawl_by_keyword(
        self, 
        keyword: str, 
        limit: int = 60,
        sort_by: str = "ctime"  # ctime, sales, price, pop
    ) -> List[Product]:
        """Crawl s·∫£n ph·∫©m theo keyword"""
        products = []
        seen_product_ids = set()  # ƒê·ªÉ tr√°nh tr√πng l·∫∑p
        
        # Map sort_by sang tham s·ªë URL c·ªßa Shopee
        sort_map = {
            "ctime": "ctime",
            "sales": "sales",
            "price": "price",
            "pop": "pop"
        }
        sort_param = sort_map.get(sort_by, "ctime")
        
        try:
            # T·∫°o URL search
            search_url = f"{self.BASE_URL}/search?keyword={keyword.replace(' ', '%20')}"
            if sort_param != "ctime":
                search_url += f"&order={sort_param}"
            
            print(f"ƒêang truy c·∫≠p: {search_url}")
            self.driver.get(search_url)
            time.sleep(5)  # ƒê·ª£i trang load ƒë·∫ßy ƒë·ªß
            
            # Debug: Ki·ªÉm tra title v√† URL
            print(f"Title: {self.driver.title}")
            print(f"Current URL: {self.driver.current_url[:100]}...")
            
            # Ki·ªÉm tra xem c√≥ b·ªã redirect ƒë·∫øn trang CAPTCHA kh√¥ng
            if '/verify/captcha' in self.driver.current_url:
                print("\n" + "="*60)
                print("‚ö†Ô∏è  SHOPEE Y√äU C·∫¶U GI·∫¢I CAPTCHA!")
                print("="*60)
                
                if not self.headless:
                    print("\nüìã H∆Ø·ªöNG D·∫™N:")
                    print("   1. Gi·∫£i CAPTCHA trong browser ƒë√£ m·ªü")
                    print("   2. Sau khi gi·∫£i xong v√† ƒë∆∞·ª£c redirect v·ªÅ trang search, nh·∫•n Enter")
                    print("   3. Tool s·∫Ω ti·∫øp t·ª•c crawl d·ªØ li·ªáu")
                    print("\n‚è≥ ƒêang ƒë·ª£i b·∫°n gi·∫£i CAPTCHA...")
                    input("\nüëâ Nh·∫•n Enter sau khi gi·∫£i CAPTCHA xong: ")
                    
                    # Ki·ªÉm tra l·∫°i URL
                    current_url = self.driver.current_url
                    if '/verify/captcha' not in current_url:
                        print("‚úÖ ƒê√£ gi·∫£i CAPTCHA th√†nh c√¥ng!")
                        # Reload trang search
                        self.driver.get(search_url)
                        time.sleep(5)
                    else:
                        print("‚ùå V·∫´n c√≤n ·ªü trang CAPTCHA. Vui l√≤ng gi·∫£i l·∫°i.")
                        return products[:limit]
                else:
                    print("\nüí° C·∫¶N GI·∫¢I CAPTCHA!")
                    print("   ƒêang t·ª± ƒë·ªông chuy·ªÉn sang ch·∫ø ƒë·ªô hi·ªÉn th·ªã browser...")
                    
                    # ƒê√≥ng browser headless v√† m·ªü l·∫°i kh√¥ng headless
                    try:
                        self.driver.quit()
                    except:
                        pass
                    
                    # M·ªü l·∫°i v·ªõi kh√¥ng headless
                    self.headless = False
                    chrome_options = Options()
                    chrome_options.add_argument('--no-sandbox')
                    chrome_options.add_argument('--disable-dev-shm-usage')
                    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
                    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
                    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
                    chrome_options.add_experimental_option('useAutomationExtension', False)
                    
                    self.driver = webdriver.Chrome(options=chrome_options)
                    self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                    self.driver.set_window_size(1920, 1080)
                    
                    # Load cookies l·∫°i
                    self._load_cookies()
                    
                    # Truy c·∫≠p l·∫°i trang search
                    self.driver.get(search_url)
                    time.sleep(3)
                    
                    # Ki·ªÉm tra l·∫°i CAPTCHA
                    if '/verify/captcha' in self.driver.current_url:
                        print("\nüìã H∆Ø·ªöNG D·∫™N GI·∫¢I CAPTCHA:")
                        print("   1. Gi·∫£i CAPTCHA trong browser ƒë√£ m·ªü")
                        print("   2. Sau khi gi·∫£i xong, nh·∫•n Enter ·ªü ƒë√¢y")
                        print("   3. Tool s·∫Ω ti·∫øp t·ª•c crawl\n")
                        input("üëâ Nh·∫•n Enter sau khi gi·∫£i CAPTCHA xong: ")
                        
                        # Ki·ªÉm tra l·∫°i
                        if '/verify/captcha' not in self.driver.current_url:
                            print("‚úÖ ƒê√£ gi·∫£i CAPTCHA th√†nh c√¥ng!")
                            self.driver.get(search_url)
                            time.sleep(5)
                        else:
                            print("‚ùå V·∫´n c√≤n ·ªü trang CAPTCHA.")
                            return products[:limit]
            
            # Ki·ªÉm tra xem c√≥ b·ªã redirect ƒë·∫øn trang login kh√¥ng
            elif '/buyer/login' in self.driver.current_url or 'login' in self.driver.title.lower():
                print("‚ö†Ô∏è Shopee y√™u c·∫ßu ƒëƒÉng nh·∫≠p!")
                print("üí° C√≥ 2 c√°ch gi·∫£i quy·∫øt:")
                print("   1. Ch·∫°y kh√¥ng headless (n) v√† ƒëƒÉng nh·∫≠p th·ªß c√¥ng trong browser")
                print("   2. Ho·∫∑c th·ª≠ truy c·∫≠p tr·ª±c ti·∫øp API v·ªõi cookies h·ª£p l·ªá")
                # Th·ª≠ ti·∫øp t·ª•c xem c√≥ th·ªÉ crawl ƒë∆∞·ª£c kh√¥ng
                print("   ƒêang th·ª≠ ti·∫øp t·ª•c...")
            
            # Ki·ªÉm tra xem c√≥ CAPTCHA kh√¥ng (fallback)
            page_text = self.driver.page_source.lower()
            if 'captcha' in page_text or 'robot' in page_text:
                if '/verify/captcha' not in self.driver.current_url:
                    print("‚ö†Ô∏è C√≥ th·ªÉ c√≥ CAPTCHA ho·∫∑c verification!")
                    print("üí° Th·ª≠ ch·∫°y kh√¥ng headless (n) ƒë·ªÉ xem browser v√† gi·∫£i CAPTCHA n·∫øu c√≥")
            
            # Debug: L∆∞u screenshot ƒë·ªÉ ki·ªÉm tra
            if not self.headless:
                try:
                    self.driver.save_screenshot("shopee_debug.png")
                    print("ƒê√£ l∆∞u screenshot: shopee_debug.png")
                except:
                    pass
            
            # Debug: Ki·ªÉm tra s·ªë l∆∞·ª£ng elements tr√™n trang
            try:
                all_divs = self.driver.find_elements(By.TAG_NAME, "div")
                all_links = self.driver.find_elements(By.TAG_NAME, "a")
                print(f"T·ªïng s·ªë divs: {len(all_divs)}, T·ªïng s·ªë links: {len(all_links)}")
            except:
                pass
            
            # Th·ª≠ l·∫•y d·ªØ li·ªáu t·ª´ network requests (API calls)
            print("ƒêang l·∫•y d·ªØ li·ªáu t·ª´ API...")
            api_products = self._crawl_from_api_keyword(keyword, limit, sort_by)
            for product in api_products:
                if product.product_id and product.product_id not in seen_product_ids:
                    products.append(product)
                    seen_product_ids.add(product.product_id)
            
            # N·∫øu ch∆∞a ƒë·ªß, th·ª≠ intercept network requests ƒë·ªÉ l·∫•y JSON
            if len(products) < limit:
                print(f"ƒê√£ l·∫•y {len(products)} s·∫£n ph·∫©m t·ª´ API, ƒëang th·ª≠ l·∫•y t·ª´ network requests...")
                network_products = self._get_products_from_network_requests(keyword, limit - len(products))
                for product in network_products:
                    if product.product_id and product.product_id not in seen_product_ids:
                        products.append(product)
                        seen_product_ids.add(product.product_id)
            
            # Ki·ªÉm tra xem c√≥ ƒëang ·ªü trang login kh√¥ng
            if '/buyer/login' in self.driver.current_url:
                print("\n" + "="*60)
                print("‚ö†Ô∏è  SHOPEE Y√äU C·∫¶U ƒêƒÇNG NH·∫¨P!")
                print("="*60)
                
                if not self.headless:
                    print("\nüìã H∆Ø·ªöNG D·∫™N:")
                    print("   1. ƒêƒÉng nh·∫≠p trong browser ƒë√£ m·ªü")
                    print("   2. Sau khi ƒëƒÉng nh·∫≠p th√†nh c√¥ng, nh·∫•n Enter ·ªü ƒë√¢y")
                    print("   3. Cookies s·∫Ω ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông ƒë·ªÉ l·∫ßn sau kh√¥ng c·∫ßn ƒëƒÉng nh·∫≠p")
                    print("\n‚è≥ ƒêang ƒë·ª£i b·∫°n ƒëƒÉng nh·∫≠p...")
                    input("\nüëâ Nh·∫•n Enter sau khi ƒëƒÉng nh·∫≠p xong: ")
                    
                    # Ki·ªÉm tra xem ƒë√£ ƒëƒÉng nh·∫≠p ch∆∞a
                    current_url = self.driver.current_url
                    if '/buyer/login' not in current_url:
                        print("‚úÖ ƒêƒÉng nh·∫≠p th√†nh c√¥ng!")
                        # L∆∞u cookies
                        self._save_cookies()
                        # Reload trang search
                        self.driver.get(search_url)
                        time.sleep(5)
                    else:
                        print("‚ùå V·∫´n ch∆∞a ƒëƒÉng nh·∫≠p. Vui l√≤ng th·ª≠ l·∫°i.")
                        return products[:limit]
                else:
                    print("\nüí° PH√ÅT HI·ªÜN C·∫¶N ƒêƒÇNG NH·∫¨P!")
                    print("   ƒêang t·ª± ƒë·ªông chuy·ªÉn sang ch·∫ø ƒë·ªô hi·ªÉn th·ªã browser...")
                    print("   (Browser s·∫Ω m·ªü ra ƒë·ªÉ b·∫°n ƒëƒÉng nh·∫≠p)\n")
                    
                    # ƒê√≥ng browser headless v√† m·ªü l·∫°i kh√¥ng headless
                    try:
                        self.driver.quit()
                    except:
                        pass
                    
                    # M·ªü l·∫°i v·ªõi kh√¥ng headless
                    self.headless = False
                    chrome_options = Options()
                    chrome_options.add_argument('--no-sandbox')
                    chrome_options.add_argument('--disable-dev-shm-usage')
                    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
                    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
                    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
                    chrome_options.add_experimental_option('useAutomationExtension', False)
                    chrome_options.set_capability('goog:loggingPrefs', {'performance': 'ALL'})
                    
                    self.driver = webdriver.Chrome(options=chrome_options)
                    self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                    self.driver.set_window_size(1920, 1080)
                    
                    # Load cookies l·∫°i
                    self._load_cookies()
                    
                    # Truy c·∫≠p l·∫°i trang search
                    self.driver.get(search_url)
                    time.sleep(3)
                    
                    # Ki·ªÉm tra l·∫°i
                    if '/buyer/login' in self.driver.current_url:
                        print("\nüìã H∆Ø·ªöNG D·∫™N ƒêƒÇNG NH·∫¨P:")
                        print("   1. ƒêƒÉng nh·∫≠p trong browser ƒë√£ m·ªü")
                        print("   2. Sau khi ƒëƒÉng nh·∫≠p th√†nh c√¥ng, nh·∫•n Enter ·ªü ƒë√¢y")
                        print("   3. Cookies s·∫Ω ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông\n")
                        input("üëâ Nh·∫•n Enter sau khi ƒëƒÉng nh·∫≠p xong: ")
                        
                        # Ki·ªÉm tra l·∫°i
                        if '/buyer/login' not in self.driver.current_url:
                            print("‚úÖ ƒêƒÉng nh·∫≠p th√†nh c√¥ng!")
                            self._save_cookies()
                            self.driver.get(search_url)
                            time.sleep(5)
                        else:
                            print("‚ùå V·∫´n ch∆∞a ƒëƒÉng nh·∫≠p.")
                            return products[:limit]
                    else:
                        print("‚úÖ ƒê√£ bypass login v·ªõi cookies!")
            
            # N·∫øu v·∫´n ch∆∞a ƒë·ªß, parse t·ª´ HTML b·∫±ng Selenium
            if len(products) < limit:
                print(f"ƒê√£ l·∫•y {len(products)} s·∫£n ph·∫©m, ƒëang parse t·ª´ HTML...")
                
                # Ki·ªÉm tra l·∫°i xem c√≥ ph·∫£i trang login kh√¥ng
                if '/buyer/login' in self.driver.current_url:
                    print("‚ùå V·∫´n ·ªü trang login. Vui l√≤ng ƒëƒÉng nh·∫≠p ho·∫∑c ch·∫°y kh√¥ng headless ƒë·ªÉ ƒëƒÉng nh·∫≠p.")
                    return products[:limit]
                
                # ƒê·ª£i trang load ho√†n to√†n
                try:
                    WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                except:
                    pass
                
                # ƒê·ª£i th√™m ƒë·ªÉ JavaScript render
                time.sleep(5)
                
                # Scroll ƒë·ªÉ load th√™m s·∫£n ph·∫©m
                scroll_pause_time = 2
                scroll_count = 0
                max_scrolls = 5
                
                while len(products) < limit and scroll_count < max_scrolls:
                    # Scroll xu·ªëng t·ª´ng ph·∫ßn
                    for i in range(3):
                        self.driver.execute_script(f"window.scrollTo(0, {i * 500});")
                        time.sleep(0.5)
                    
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(scroll_pause_time)
                    scroll_count += 1
                    
                    # Debug: In ra s·ªë l∆∞·ª£ng links t√¨m th·∫•y
                    try:
                        all_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/product/']")
                        print(f"T√¨m th·∫•y {len(all_links)} links s·∫£n ph·∫©m...")
                    except:
                        pass
                    
                    # D√πng Selenium ƒë·ªÉ t√¨m elements tr·ª±c ti·∫øp
                    try:
                        # Th·ª≠ nhi·ªÅu selector kh√°c nhau
                        selectors = [
                            "a[href*='/product/']",
                            "div[class*='shopee-search-item-result'] a",
                            "div[class*='col-xs-2-4'] a",
                            "div[data-sqe='item'] a",
                            "[class*='product-item'] a",
                            "[class*='search-result'] a"
                        ]
                        
                        for selector in selectors:
                            try:
                                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                                print(f"Selector '{selector}': t√¨m th·∫•y {len(elements)} elements")
                                
                                for elem in elements:
                                    if len(products) >= limit:
                                        break
                                    try:
                                        product = self._parse_product_from_selenium_element(elem)
                                        if product and product.product_id and product.product_id not in seen_product_ids:
                                            products.append(product)
                                            seen_product_ids.add(product.product_id)
                                            print(f"ƒê√£ parse s·∫£n ph·∫©m: {product.name[:50]}...")
                                    except Exception as e:
                                        continue
                                
                                if len(products) >= limit:
                                    break
                            except Exception as e:
                                continue
                    except Exception as e:
                        print(f"L·ªói khi parse HTML: {e}")
                    
                    if len(products) >= limit:
                        break
                    
                    # N·∫øu kh√¥ng t√¨m th·∫•y g√¨ sau scroll ƒë·∫ßu ti√™n, th·ª≠ c√°ch kh√°c
                    if scroll_count == 1 and len(products) == 0:
                        print("Th·ª≠ c√°ch parse kh√°c...")
                        # Th·ª≠ parse t·ª´ HTML source
                        html = self.driver.page_source
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # T√¨m t·∫•t c·∫£ links c√≥ ch·ª©a /product/
                        product_links = soup.find_all('a', href=re.compile(r'/product/\d+/\d+'))
                        print(f"T√¨m th·∫•y {len(product_links)} product links trong HTML")
                        
                        for link in product_links[:limit]:
                            try:
                                href = link.get('href', '')
                                match = re.search(r'/product/(\d+)/(\d+)', href)
                                if match:
                                    shop_id = match.group(1)
                                    product_id = match.group(2)
                                    
                                    if product_id not in seen_product_ids:
                                        # T√¨m parent element ƒë·ªÉ l·∫•y th√¥ng tin
                                        parent = link.find_parent()
                                        name = ""
                                        price = 0
                                        
                                        if parent:
                                            name_elem = parent.find(string=re.compile(r'.+'))
                                            if name_elem:
                                                name = name_elem.strip()[:200]
                                        
                                        if not name:
                                            name = link.get_text(strip=True)[:200]
                                        
                                        if name:
                                            product = Product(
                                                name=name,
                                                price=price,
                                                original_price=None,
                                                commission_rate=None,
                                                sales_count=0,
                                                rating=None,
                                                shop_name="",
                                                shop_id=shop_id,
                                                product_id=product_id,
                                                category="",
                                                image_url="",
                                                product_url=f"{self.BASE_URL}{href}" if href.startswith('/') else href,
                                                location=""
                                            )
                                            products.append(product)
                                            seen_product_ids.add(product_id)
                                            print(f"ƒê√£ parse t·ª´ HTML: {name[:50]}...")
                            except:
                                continue
                        
                        if len(products) > 0:
                            break
            
        except Exception as e:
            print(f"L·ªói khi crawl keyword {keyword}: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"ƒê√£ crawl ƒë∆∞·ª£c {len(products)} s·∫£n ph·∫©m")
        return products[:limit]
    
    def _crawl_from_api_keyword(self, keyword: str, limit: int, sort_by: str) -> List[Product]:
        """Th·ª≠ crawl t·ª´ API v·ªõi cookies t·ª´ Selenium"""
        products = []
        try:
            # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o cookies ƒë√£ ƒë∆∞·ª£c set
            time.sleep(2)
            
            # L·∫•y cookies t·ª´ Selenium
            cookies = self.driver.get_cookies()
            session = requests.Session()
            for cookie in cookies:
                session.cookies.set(cookie['name'], cookie['value'], domain=cookie.get('domain', '.shopee.vn'))
            
            # L·∫•y c√°c headers t·ª´ browser
            user_agent = self.driver.execute_script("return navigator.userAgent;")
            
            # Encode keyword ƒë√∫ng c√°ch
            encoded_keyword = keyword.replace(" ", "%20")
            try:
                import urllib.parse
                encoded_keyword = urllib.parse.quote(keyword)
            except:
                pass
            
            session.headers.update({
                'User-Agent': user_agent,
                'Referer': f'{self.BASE_URL}/search?keyword={encoded_keyword}',
                'Accept': 'application/json',
                'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
                'X-Requested-With': 'XMLHttpRequest',
                'X-API-Source': 'pc',
                'X-Shopee-Language': 'vi',
            })
            
            # Th·ª≠ g·ªçi API v·ªõi cookies
            api_url = "https://shopee.vn/api/v4/search/search_items"
            page = 0
            
            while len(products) < limit:
                # Encode keyword ƒë√∫ng c√°ch
                try:
                    import urllib.parse
                    encoded_keyword = urllib.parse.quote(keyword)
                except:
                    encoded_keyword = keyword.replace(" ", "%20")
                
                params = {
                    'by': sort_by,
                    'keyword': encoded_keyword,
                    'limit': min(60, limit - len(products)),
                    'newest': page * 60,
                    'order': 'desc' if sort_by != 'price' else 'asc',
                    'page_type': 'search',
                    'scenario': 'PAGE_GLOBAL_SEARCH',
                    'version': 2
                }
                
                # Encode params ƒë√∫ng c√°ch
                try:
                    response = session.get(api_url, params=params, timeout=15)
                except UnicodeEncodeError:
                    # Fallback: encode manually
                    import urllib.parse
                    query_string = urllib.parse.urlencode(params, quote_via=urllib.parse.quote)
                    full_url = f"{api_url}?{query_string}"
                    response = session.get(full_url, timeout=15)
                
                if response.status_code == 200:
                    data = response.json()
                    items = data.get('items', [])
                    if not items:
                        break
                    for item in items:
                        if len(products) >= limit:
                            break
                        product = self._parse_product_from_api(item)
                        if product:
                            products.append(product)
                    page += 1
                    time.sleep(1)
                else:
                    # N·∫øu b·ªã 403, th·ª≠ intercept network requests t·ª´ Selenium
                    if response.status_code == 403:
                        print("API b·ªã ch·∫∑n, s·∫Ω parse t·ª´ HTML...")
                    break
        except Exception as e:
            print(f"L·ªói khi crawl t·ª´ API: {e}")
        
        return products
    
    def _get_products_from_network_requests(self, keyword: str, limit: int) -> List[Product]:
        """L·∫•y d·ªØ li·ªáu t·ª´ network requests b·∫±ng Chrome DevTools Protocol"""
        products = []
        try:
            # L·∫•y performance logs ƒë·ªÉ xem network requests
            logs = self.driver.get_log('performance')
            
            for log in logs:
                try:
                    message = json.loads(log['message'])['message']
                    if message['method'] == 'Network.responseReceived':
                        url = message['params']['response']['url']
                        if 'search_items' in url or 'search/search_items' in url:
                            request_id = message['params']['requestId']
                            # L·∫•y response body
                            try:
                                response_body = self.driver.execute_cdp_cmd('Network.getResponseBody', {'requestId': request_id})
                                if response_body and 'body' in response_body:
                                    data = json.loads(response_body['body'])
                                    if 'items' in data:
                                        for item in data['items'][:limit]:
                                            product = self._parse_product_from_api(item)
                                            if product:
                                                products.append(product)
                            except:
                                pass
                except:
                    continue
            
            # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ logs, th·ª≠ intercept b·∫±ng JavaScript
            if len(products) == 0:
                # Scroll ƒë·ªÉ trigger API calls
                for i in range(2):
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(2)
                
                # Th·ª≠ l·∫•y t·ª´ window object
                try:
                    script = """
                    return window.__NEXT_DATA__ || window.__INITIAL_STATE__ || window.__SHOPEE_DATA__ || {};
                    """
                    data = self.driver.execute_script(script)
                    if isinstance(data, dict):
                        # T√¨m items trong data
                        def find_items(obj, path=""):
                            if isinstance(obj, dict):
                                if 'items' in obj and isinstance(obj['items'], list):
                                    return obj['items']
                                for key, value in obj.items():
                                    result = find_items(value, f"{path}.{key}")
                                    if result:
                                        return result
                            elif isinstance(obj, list):
                                for item in obj:
                                    result = find_items(item, path)
                                    if result:
                                        return result
                            return None
                        
                        items = find_items(data)
                        if items:
                            for item in items[:limit]:
                                product = self._parse_product_from_api(item)
                                if product:
                                    products.append(product)
                except Exception as e:
                    print(f"L·ªói khi l·∫•y t·ª´ window object: {e}")
                
        except Exception as e:
            print(f"L·ªói khi l·∫•y t·ª´ network: {e}")
        
        return products[:limit]
    
    def _parse_product_from_selenium_element(self, element) -> Optional[Product]:
        """Parse s·∫£n ph·∫©m t·ª´ Selenium WebElement"""
        try:
            # L·∫•y href ƒë·ªÉ extract product_id
            href = element.get_attribute('href')
            
            # N·∫øu element kh√¥ng ph·∫£i l√† link, t√¨m link b√™n trong
            if not href or '/product/' not in href:
                try:
                    # Th·ª≠ t√¨m link trong element ho·∫∑c parent
                    link_elem = element.find_element(By.CSS_SELECTOR, "a[href*='/product/']")
                    href = link_elem.get_attribute('href')
                except:
                    try:
                        # Th·ª≠ t√¨m trong parent
                        parent = element.find_element(By.XPATH, "./..")
                        link_elem = parent.find_element(By.CSS_SELECTOR, "a[href*='/product/']")
                        href = link_elem.get_attribute('href')
                    except:
                        return None
            
            if not href or '/product/' not in href:
                return None
            
            # Extract shop_id v√† product_id
            match = re.search(r'/product/(\d+)/(\d+)', href)
            if not match:
                return None
            
            shop_id = match.group(1)
            product_id = match.group(2)
            product_url = f"{self.BASE_URL}{href}" if href.startswith('/') else href
            
            # L·∫•y t√™n s·∫£n ph·∫©m
            name = ""
            try:
                name_elem = element.find_element(By.CSS_SELECTOR, "[class*='name'], [class*='title'], [class*='product-name']")
                name = name_elem.text.strip()
            except:
                try:
                    name = element.find_element(By.TAG_NAME, "a").text.strip()
                except:
                    pass
            
            if not name:
                return None
            
            # L·∫•y gi√°
            price = 0
            try:
                price_elem = element.find_element(By.CSS_SELECTOR, "[class*='price'], [class*='final-price']")
                price_text = price_elem.text.strip()
                price_match = re.search(r'(\d+(?:\.\d+)?)', price_text.replace('.', '').replace(',', ''))
                if price_match:
                    price = float(price_match.group(1))
            except:
                pass
            
            # L·∫•y h√¨nh ·∫£nh
            image_url = ""
            try:
                img_elem = element.find_element(By.CSS_SELECTOR, "img")
                image_url = img_elem.get_attribute('src') or img_elem.get_attribute('data-src') or ""
                if image_url and not image_url.startswith('http'):
                    image_url = f"https:{image_url}" if image_url.startswith('//') else f"https://{image_url}"
            except:
                pass
            
            # L·∫•y s·ªë l∆∞·ª£ng b√°n
            sales_count = 0
            try:
                text = element.text
                sold_match = re.search(r'ƒë√£\s*b√°n[:\s]*(\d+(?:\.\d+)?[kK]?)', text, re.IGNORECASE)
                if sold_match:
                    sold_num = sold_match.group(1).lower()
                    if 'k' in sold_num:
                        sales_count = int(float(sold_num.replace('k', '')) * 1000)
                    else:
                        sales_count = int(float(sold_num))
            except:
                pass
            
            return Product(
                name=name,
                price=price,
                original_price=None,
                commission_rate=None,
                sales_count=sales_count,
                rating=None,
                shop_name="",
                shop_id=shop_id,
                product_id=product_id,
                category="",
                image_url=image_url,
                product_url=product_url,
                location=""
            )
        except Exception as e:
            return None
    
    def crawl_by_category(
        self,
        category_id: int,
        limit: int = 60,
        sort_by: str = "ctime"
    ) -> List[Product]:
        """Crawl s·∫£n ph·∫©m theo category"""
        products = []
        
        try:
            category_url = f"{self.BASE_URL}/api/v4/search/search_items"
            
            # L·∫•y cookies t·ª´ Selenium
            cookies = self.driver.get_cookies()
            session = requests.Session()
            for cookie in cookies:
                session.cookies.set(cookie['name'], cookie['value'])
            
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Referer': f'{self.BASE_URL}/',
                'Accept': 'application/json',
            })
            
            page = 0
            while len(products) < limit:
                params = {
                    'by': sort_by,
                    'categoryids': category_id,
                    'limit': min(60, limit - len(products)),
                    'newest': page * 60,
                    'order': 'desc' if sort_by != 'price' else 'asc',
                    'page_type': 'search',
                    'scenario': 'PAGE_CATEGORY',
                    'version': 2
                }
                
                response = session.get(category_url, params=params, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    items = data.get('items', [])
                    if not items:
                        break
                    for item in items:
                        if len(products) >= limit:
                            break
                        product = self._parse_product_from_api(item)
                        if product:
                            products.append(product)
                    page += 1
                    time.sleep(1)
                else:
                    break
        except Exception as e:
            print(f"L·ªói khi crawl category {category_id}: {e}")
        
        return products[:limit]
    
    def crawl_by_shop(
        self,
        shop_id: str,
        limit: int = 60
    ) -> List[Product]:
        """Crawl s·∫£n ph·∫©m theo shop"""
        products = []
        
        try:
            shop_url = f"{self.BASE_URL}/shop/{shop_id}"
            print(f"ƒêang truy c·∫≠p shop: {shop_url}")
            self.driver.get(shop_url)
            time.sleep(3)
            
            # Scroll v√† load s·∫£n ph·∫©m
            scroll_pause_time = 1
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            
            while len(products) < limit:
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(scroll_pause_time)
                
                html = self.driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                # Parse s·∫£n ph·∫©m t·ª´ HTML
                product_elements = soup.find_all('div', class_=re.compile(r'col-xs-2-4|shopee-search-item'))
                for element in product_elements:
                    if len(products) >= limit:
                        break
                    product = self._parse_product_from_html(element, shop_id=shop_id)
                    if product and product not in products:
                        products.append(product)
                
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height
                
                if len(products) >= limit:
                    break
        except Exception as e:
            print(f"L·ªói khi crawl shop {shop_id}: {e}")
        
        return products[:limit]
    
    def _parse_product_from_api(self, item: Dict) -> Optional[Product]:
        """Parse s·∫£n ph·∫©m t·ª´ API response"""
        try:
            item_basic = item.get('item_basic', {})
            
            if not item_basic:
                return None
            
            price = item_basic.get('price', 0) / 100000
            original_price = item_basic.get('price_before_discount', 0) / 100000
            shop_id = str(item_basic.get('shopid', ''))
            shop_name = item_basic.get('shop_name', '')
            sales_count = item_basic.get('historical_sold', 0)
            rating = item_basic.get('item_rating', {}).get('rating_star', 0)
            name = item_basic.get('name', '')
            product_id = str(item_basic.get('itemid', ''))
            image_url = f"https://cf.shopee.vn/file/{item_basic.get('image', '')}"
            product_url = f"https://shopee.vn/product/{shop_id}/{product_id}"
            category = str(item_basic.get('catid', ''))
            location = item_basic.get('shop_location', '')
            
            return Product(
                name=name,
                price=price,
                original_price=original_price if original_price > price else None,
                commission_rate=None,
                sales_count=sales_count,
                rating=rating,
                shop_name=shop_name,
                shop_id=shop_id,
                product_id=product_id,
                category=category,
                image_url=image_url,
                product_url=product_url,
                location=location
            )
        except Exception as e:
            return None
    
    def _parse_product_from_html(self, element, shop_id: Optional[str] = None) -> Optional[Product]:
        """Parse s·∫£n ph·∫©m t·ª´ HTML element"""
        try:
            # T√¨m link s·∫£n ph·∫©m tr∆∞·ªõc (quan tr·ªçng nh·∫•t)
            link_elem = element.find('a', href=re.compile(r'/product/'))
            if not link_elem:
                return None
            
            href = link_elem.get('href', '')
            if not href:
                return None
            
            product_url = f"{self.BASE_URL}{href}" if href.startswith('/') else href
            
            # Extract shop_id v√† product_id t·ª´ URL
            match = re.search(r'/product/(\d+)/(\d+)', href)
            if not match:
                return None
            
            shop_id = match.group(1)
            product_id = match.group(2)
            
            # T√¨m t√™n s·∫£n ph·∫©m - th·ª≠ nhi·ªÅu selector
            name = ""
            name_selectors = [
                'div[class*="name"]',
                'div[class*="product-name"]',
                'div[class*="title"]',
                'a[href*="/product/"]'
            ]
            for selector in name_selectors:
                name_elem = element.select_one(selector)
                if name_elem:
                    name = name_elem.get_text(strip=True)
                    if name:
                        break
            
            if not name:
                name = link_elem.get_text(strip=True)
            
            if not name:
                return None
            
            # T√¨m gi√° - th·ª≠ nhi·ªÅu selector
            price = 0
            price_selectors = [
                'span[class*="price"]',
                'div[class*="price"]',
                '[class*="final-price"]',
                '[class*="current-price"]'
            ]
            for selector in price_selectors:
                price_elem = element.select_one(selector)
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    # Extract s·ªë t·ª´ gi√°
                    price_match = re.search(r'(\d+(?:\.\d+)?)', price_text.replace('.', '').replace(',', ''))
                    if price_match:
                        price = float(price_match.group(1))
                        break
            
            # T√¨m h√¨nh ·∫£nh
            img_elem = element.find('img')
            image_url = ""
            if img_elem:
                image_url = img_elem.get('src', '') or img_elem.get('data-src', '')
                if image_url and not image_url.startswith('http'):
                    image_url = f"https:{image_url}" if image_url.startswith('//') else f"https://{image_url}"
            
            # T√¨m s·ªë l∆∞·ª£ng b√°n
            sales_count = 0
            sold_text = element.get_text()
            sold_match = re.search(r'ƒë√£\s*b√°n[:\s]*(\d+(?:\.\d+)?[kK]?)', sold_text, re.IGNORECASE)
            if sold_match:
                sold_num = sold_match.group(1).lower()
                if 'k' in sold_num:
                    sales_count = int(float(sold_num.replace('k', '')) * 1000)
                else:
                    sales_count = int(float(sold_num))
            
            # T√¨m rating
            rating = None
            rating_elem = element.find(string=re.compile(r'\d+\.\d+'))
            if rating_elem:
                rating_match = re.search(r'(\d+\.\d+)', rating_elem)
                if rating_match:
                    rating = float(rating_match.group(1))
            
            return Product(
                name=name,
                price=price,
                original_price=None,
                commission_rate=None,
                sales_count=sales_count,
                rating=rating,
                shop_name="",
                shop_id=shop_id,
                product_id=product_id,
                category="",
                image_url=image_url,
                product_url=product_url,
                location=""
            )
        except Exception as e:
            return None
